{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# === ê²½ë¡œ ì„¤ì • ===\n",
    "zip_root = r\"C:\\AIhub\\Training\"  # .zip íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆëŠ” ìµœìƒìœ„ í´ë”\n",
    "target_root_01 = os.path.join(zip_root, \"01.ì›ì²œë°ì´í„°\")\n",
    "target_root_02 = os.path.join(zip_root, \"02.ë¼ë²¨ë§ë°ì´í„°\")\n",
    "\n",
    "# === í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± ===\n",
    "os.makedirs(target_root_01, exist_ok=True)\n",
    "os.makedirs(target_root_02, exist_ok=True)\n",
    "\n",
    "# === ì••ì¶• í•´ì œ ì‹¤í–‰ ===\n",
    "count = 0\n",
    "for root, _, files in os.walk(zip_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".zip\"):\n",
    "            zip_path = os.path.join(root, file)\n",
    "\n",
    "            # ì–´ë–¤ ëŒ€ìƒ í´ë”ë¡œ ë³´ë‚¼ì§€ êµ¬ë¶„ (TS â†’ ì›ì²œë°ì´í„° / TL â†’ ë¼ë²¨ë§ë°ì´í„°)\n",
    "            if file.startswith(\"TS_\"):\n",
    "                target_dir = target_root_01\n",
    "            elif file.startswith(\"TL_\"):\n",
    "                target_dir = target_root_02\n",
    "            else:\n",
    "                print(f\"â— ë¬´ì‹œ: {file} (TS_ ë˜ëŠ” TL_ë¡œ ì‹œì‘ ì•ˆ í•¨)\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    for member in zip_ref.namelist():\n",
    "                        if member.endswith(\".json\"):\n",
    "                            # í•˜ìœ„ ë””ë ‰í† ë¦¬ ë¬´ì‹œí•˜ê³  íŒŒì¼ë§Œ ì¶”ì¶œ\n",
    "                            file_name = os.path.basename(member)\n",
    "                            target_path = os.path.join(target_dir, file_name)\n",
    "                            with zip_ref.open(member) as source_file, open(target_path, 'wb') as target_file:\n",
    "                                target_file.write(source_file.read())\n",
    "                print(f\"ğŸ“¦ ì••ì¶• í•´ì œ ì™„ë£Œ: {file}\")\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì‹¤íŒ¨: {file} - {e}\")\n",
    "\n",
    "print(f\"\\nâœ… ì´ {count}ê°œì˜ zip ì••ì¶• ì™„ë£Œ, ëª¨ë“  íŒŒì¼ì´ ì›ì²œ/ë¼ë²¨ë§ í´ë”ë¡œ ì§ì ‘ ë“¤ì–´ê°”ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# === ê²½ë¡œ ì„¤ì • ===\n",
    "source_root = r\"C:\\AIhub\\Training\\01.ì›ì²œë°ì´í„°\"\n",
    "label_root  = r\"C:\\AIhub\\Training\\02.ë¼ë²¨ë§ë°ì´í„°\"\n",
    "output_csv  = r\"C:\\AIhub\\train_data.csv\"\n",
    "\n",
    "data = []\n",
    "matched = 0\n",
    "skipped = 0\n",
    "error_files = []\n",
    "\n",
    "# === 1. ë¼ë²¨ íŒŒì¼ ë”•ì…”ë„ˆë¦¬ ìƒì„± (_L.json â†’ ì›ì²œ í‚¤) ===\n",
    "label_map = {\n",
    "    file.replace(\"_L.json\", \"\").strip().lower(): os.path.join(label_root, file)\n",
    "    for file in os.listdir(label_root)\n",
    "    if file.endswith(\"_L.json\")\n",
    "}\n",
    "\n",
    "# === 2. ì „ì²´ ì›ì²œ íŒŒì¼ ìˆ˜ì§‘ ===\n",
    "source_files = [f for f in os.listdir(source_root) if f.endswith(\".json\")]\n",
    "total = len(source_files)\n",
    "\n",
    "print(f\"ğŸ” ì›ì²œ íŒŒì¼ ì´ {total}ê°œ ë³‘í•© ì‹œì‘...\")\n",
    "\n",
    "# === 3. ë³‘í•© ë£¨í”„ ===\n",
    "for i, file in enumerate(source_files, 1):\n",
    "    file_key = file.replace(\".json\", \"\").strip().lower()\n",
    "    source_path = os.path.join(source_root, file)\n",
    "    label_path = label_map.get(file_key)\n",
    "\n",
    "    if not label_path or not os.path.exists(label_path):\n",
    "        skipped += 1\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"[{i}/{total}] â— ëˆ„ë½ ëˆ„ì : {skipped}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "\n",
    "        with open(source_path, \"r\", encoding=\"utf-8\") as f_src:\n",
    "            source = json.load(f_src)\n",
    "        with open(label_path, \"r\", encoding=\"utf-8\") as f_lbl:\n",
    "            label = json.load(f_lbl)\n",
    "\n",
    "        t2 = time.time()\n",
    "        if t2 - t1 > 1.0:\n",
    "            print(f\"â± ëŠë¦¼: {file} - {round(t2 - t1, 2)}ì´ˆ\")\n",
    "\n",
    "        title = source.get(\"sourceDataInfo\", {}).get(\"newsTitle\", \"\")\n",
    "        content = source.get(\"sourceDataInfo\", {}).get(\"newsContent\", \"\")\n",
    "        label_value = label.get(\"labeledDataInfo\", {}).get(\"clickbaitClass\", None)\n",
    "\n",
    "        if label_value in [0, 1]:\n",
    "            data.append({\n",
    "                \"text\": f\"{title} {content}\".strip(),\n",
    "                \"label\": label_value\n",
    "            })\n",
    "            matched += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        error_files.append((file, str(e)))\n",
    "        skipped += 1\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"[{i}/{total}] ì§„í–‰ ì¤‘... âœ… {matched} ì„±ê³µ / âŒ {skipped} ì‹¤íŒ¨\")\n",
    "\n",
    "# === 4. ì €ì¥ ===\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²´ ë³‘í•© ì™„ë£Œ: {matched}ê±´ ì„±ê³µ, {skipped}ê±´ ì‹¤íŒ¨/ëˆ„ë½\")\n",
    "print(f\"ğŸ“„ ì €ì¥ ìœ„ì¹˜: {output_csv}\")\n",
    "\n",
    "if error_files:\n",
    "    print(f\"âš ï¸ ì˜ˆì™¸ ë°œìƒ íŒŒì¼ ì˜ˆì‹œ: {error_files[:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… pandas import ì‹œì‘\")\n",
    "import pandas as pd\n",
    "print(\"âœ… pandas import ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸ“¥ CSV íŒŒì¼ ë¡œë“œ ì¤‘...\")\n",
    "df = pd.read_csv(r\"C:\\AIhub\\train_data.csv\", encoding=\"utf-8\")\n",
    "print(\"âœ… CSV ë¡œë”© ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸ“¥ sklearn, matplotlib ë“± í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì¤‘...\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸ”§ ë°ì´í„° í”„ë ˆì„ ì»¬ëŸ¼ ì†Œë¬¸ì ë° ê³µë°± ì œê±° ì¤‘...\")\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "print(f\"âœ… ì»¬ëŸ¼ëª… ì²˜ë¦¬ ì™„ë£Œ: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"ğŸ” í…ìŠ¤íŠ¸ ë° ë¼ë²¨ ì—´ ìë™ ê°ì§€ ì¤‘...\")\n",
    "text_col = next((col for col in df.columns if 'text' in col), None)\n",
    "label_col = next((col for col in df.columns if 'label' in col or 'class' in col), None)\n",
    "\n",
    "if not text_col or not label_col:\n",
    "    raise ValueError(f\"âŒ 'text' ë˜ëŠ” 'label' ê´€ë ¨ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\ní˜„ì¬ ì—´ ëª©ë¡: {df.columns.tolist()}\")\n",
    "print(f\"âœ… í…ìŠ¤íŠ¸ ì—´: '{text_col}', ë¼ë²¨ ì—´: '{label_col}' ê°ì§€ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•œ ìƒ˜í”Œë§ (2000ê°œê¹Œì§€ë§Œ ì‚¬ìš©)\n",
    "print(\"ğŸ“‰ ë°ì´í„° ì¼ë¶€ ìƒ˜í”Œë§ ì¤‘ (ìµœëŒ€ 2000ê°œ)...\")\n",
    "df_sample = df[[text_col, label_col]].dropna().sample(n=2000, random_state=42) if len(df) > 2000 else df\n",
    "print(f\"âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(df_sample)}ê°œ\")\n",
    "\n",
    "print(\"ğŸ”€ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  ì¤‘...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sample[text_col], df_sample[label_col], test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ: í•™ìŠµ {len(X_train)}ê°œ, ê²€ì¦ {len(X_test)}ê°œ\")\n",
    "\n",
    "print(\"ğŸ”¤ TF-IDF ë²¡í„°í™” ì¤‘...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # ì°¨ì› ì¶•ì†Œ\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "print(\"âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ\")\n",
    "\n",
    "print(\"âš™ï¸ SVM ëª¨ë¸ í•™ìŠµ ì¤‘ (ì„ í˜• ì»¤ë„)...\")\n",
    "model = SVC(kernel=\"linear\")  # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•œ ì„ í˜• ì»¤ë„\n",
    "model.fit(X_train_vec, y_train)\n",
    "print(\"âœ… SVM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸ” ì˜ˆì¸¡ ë° í‰ê°€ ì¤‘...\")\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"âœ… ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸ“Š TF-IDF + SVM ë¶„ë¥˜ ê²°ê³¼\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"ğŸ“ˆ í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ì¤‘...\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"ì •ìƒ\", \"ì–´ë·°ì§•\"], yticklabels=[\"ì •ìƒ\", \"ì–´ë·°ì§•\"])\n",
    "plt.title(\"TF-IDF + SVM Confusion Matrix\")\n",
    "plt.xlabel(\"ì˜ˆì¸¡\")\n",
    "plt.ylabel(\"ì‹¤ì œ\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"âœ… ì‹œê°í™” ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "print(\"ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "df = pd.read_csv(r\"C:\\AIhub\\train_data.csv\", encoding='utf-8', engine=\"python\")\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df[['text', 'label']].dropna()\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# 2. Dataset í¬ë§· ë³€í™˜\n",
    "print(\"ğŸ“¦ 2ë‹¨ê³„: HuggingFace Dataset í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì¤‘...\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(\"âœ… ë³€í™˜ ë° í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ\")\n",
    "\n",
    "# 3. Tokenizer ì¤€ë¹„\n",
    "print(\"ğŸ”¤ 3ë‹¨ê³„: Tokenizer ë¡œë”© ë° ì ìš© ì¤‘...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(\"âœ… í† í°í™” ì™„ë£Œ\")\n",
    "\n",
    "# 4. ëª¨ë¸ ì¤€ë¹„\n",
    "print(\"ğŸ¤– 4ë‹¨ê³„: BERT ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "\n",
    "# 5. í•™ìŠµ ì„¤ì •\n",
    "print(\"âš™ï¸ 5ë‹¨ê³„: í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • ì¤‘...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "print(\"âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "# 6. Trainer ì„¤ì •\n",
    "print(\"ğŸš€ 6ë‹¨ê³„: Trainer ì„¤ì • ì¤‘...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "print(\"âœ… Trainer ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "# 7. í•™ìŠµ ì‹œì‘\n",
    "print(\"ğŸ§  7ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "trainer.train()\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "# 8. ì˜ˆì¸¡ ë° í‰ê°€\n",
    "print(\"ğŸ” 8ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
    "preds = trainer.predict(tokenized_dataset[\"test\"])\n",
    "y_pred = torch.argmax(torch.tensor(preds.predictions), axis=1).numpy()\n",
    "y_true = tokenized_dataset[\"test\"][\"labels\"]\n",
    "\n",
    "print(\"ğŸ“Š BERT ë¶„ë¥˜ ì„±ëŠ¥ ê²°ê³¼:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# 9. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
    "print(\"ğŸ“‰ 9ë‹¨ê³„: í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ì¤‘...\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=[\"Normal\", \"Abusing\"], yticklabels=[\"Normal\", \"Abusing\"])\n",
    "plt.title(\"BERT Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
